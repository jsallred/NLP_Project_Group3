{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "733893be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95fc4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scraped_data/BABE_scraped.csv')\n",
    "df['content'] = df['content'].str.lower()  # Convert text to lowercase\n",
    "\n",
    "df.dropna(subset=['content'], inplace=True) # Drop rows with missing values in the 'content' column\n",
    "df['type_class'] = LabelEncoder().fit_transform(df['type_class'])\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_val_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c647570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "def encode_text(text, word2id):\n",
    "    return [word2id.get(word, word2id['<pad>']) for word in text.split()]\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data, word2id):\n",
    "        self.data = data\n",
    "        self.word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['content']\n",
    "        label = self.data.iloc[idx]['type_class']\n",
    "        encoded_text = encode_text(text, self.word2id)\n",
    "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_text)\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(label_list, dtype=torch.long)\n",
    "    return text_list, labels\n",
    "\n",
    "# Create datasets and dataloaders for train, validation, and test sets\n",
    "train_dataset = NewsDataset(train_data, word2id)\n",
    "val_dataset = NewsDataset(val_data, word2id)\n",
    "test_dataset = NewsDataset(test_data, word2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c84b74b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words from GloVe.\n"
     ]
    }
   ],
   "source": [
    "glove_file = \"../Homeworks/HW2_Language_Models_Neural_Networks/glove/glove.6B.50d.txt\"\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "print(f'Loaded {len(embeddings_dict)} words from GloVe.')\n",
    "\n",
    "# Create a matrix for the word vectors\n",
    "vocab_size = len(embeddings_dict) + 1\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "word2id = {'<pad>': 0}\n",
    "for i, (word, vector) in enumerate(embeddings_dict.items(), 1):\n",
    "    embedding_matrix[i] = vector\n",
    "    word2id[word] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37e31970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pretrained_embeddings):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False  # We do not train the embedding layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(1, x.size(0), self.rnn.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Pass the output of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b243129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.7844, Validation Loss: 0.8999\n",
      "Epoch [2/10], Training Loss: 1.0469, Validation Loss: 0.9298\n",
      "Epoch [3/10], Training Loss: 0.9587, Validation Loss: 0.8998\n",
      "Epoch [4/10], Training Loss: 0.9311, Validation Loss: 1.0338\n",
      "Epoch [5/10], Training Loss: 1.1376, Validation Loss: 0.9605\n",
      "Epoch [6/10], Training Loss: 0.8903, Validation Loss: 0.9115\n",
      "Epoch [7/10], Training Loss: 0.9706, Validation Loss: 0.9323\n",
      "Epoch [8/10], Training Loss: 1.0352, Validation Loss: 0.9816\n",
      "Epoch [9/10], Training Loss: 1.1408, Validation Loss: 0.8975\n",
      "Epoch [10/10], Training Loss: 0.8828, Validation Loss: 0.9070\n"
     ]
    }
   ],
   "source": [
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim=50, output_dim=3, pretrained_embeddings=embedding_matrix)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for texts, labels in train_loader:\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            outputs = model(texts)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss / len(val_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6160aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 50.46153846153846%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test set: {100 * correct / total}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
